# Vigía Innovation Lab Framework (VILF) 1.0

## 00 – Executive Summary

### 0.1 Purpose of VILF
The Vigía Innovation Lab Framework (VILF) provides a complete, structured methodology for designing, deploying, governing, scaling, and sustaining innovation labs within a national or multi-institutional ecosystem. It defines the architectural components, operational systems, funding structures, capability requirements, governance pathways, and performance mechanisms required to build a coherent innovation lab network.

VILF is part of the Vigía Framework Family (VXF) and is fully aligned with:
- MicroCanvas Framework (MCF 2.1)
- Innovation Maturity Model (IMM)
- IMM-P® execution cycle
- Innovation Governance Framework (IGF)
- Monitoring, Evaluation & Learning (MEL)

### 0.2 What VILF Enables
VILF allows countries or institutions to:
- build innovation labs that operate predictably,
- scale from isolated pilots to robust networks,
- integrate evidence and learning across institutions,
- establish a shared governance and performance system,
- align innovation with public value, strategic outcomes, and policy impact,
- professionalize the innovation function.

### 0.3 Who Should Use This Framework
VILF is designed for:
- governments establishing national innovation lab networks,
- universities and multi-institutional coalitions launching shared labs,
- private-sector groups building distributed innovation units,
- international organizations supporting national innovation ecosystems.

### 0.4 Structure of VILF
VILF is composed of 16 components across Chapters 00–15:
- Executive Summary  
- Introduction  
- Ecosystem Diagnostic  
- System Architecture  
- Operating Model  
- Funding Model  
- Benchmarking  
- KPIs & Scorecard  
- Roadmap & Phasing  
- Governance & Legal Toolkit  
- Templates & Tools  
- References  
- License  
- Release Notes  
- Roadmap (Future Versions)  
- Annexes  

### 0.5 How to Read This Document
This document may be read:
- linearly (Chapters 00–15),
- by functional domain (architecture, governance, funding, MEL),
- or by role (lab manager, hub coordinator, NCU director).

Each chapter ends with a connection block linking to the next chapter.

---

## 00a – How the Framework Works (Usability Overview)

### 00a.1 Conceptual Model
VILF is built using a three-tier architecture:
- **Tier-0: Labs** → execute innovation cycles and generate evidence.
- **Tier-1: Hubs** → coordinate multiple labs, build capability, manage portfolios.
- **Tier-2: Network Coordination Unit (NCU)** → governs standards, MEL, accreditation.

These tiers operate as a single interoperable system.

### 00a.2 Chapter Map (Corrected)
1. Executive Summary  
2. Introduction  
3. Ecosystem Diagnostic  
4. System Architecture  
5. Operating Model  
6. Funding Model  
7. Benchmarking  
8. KPIs & Scorecard (MEL)  
9. Roadmap & Phasing  
10. Governance & Legal Toolkit  
11. Templates & Tools  
12. References  
13. License  
14. Release Notes  
15. Roadmap (Future Versions)  
16. Annexes  

### 00a.3 Core Spine Definitions
- **Execution Spine:** IMM-P®  
- **Governance Spine:** IGF  
- **Capability Spine:** IMM  
- **Performance Spine:** MEL  
- **Infrastructure Spine:** evidence repository + documentation system  

### 00a.4 Connection to Introduction
The next chapter introduces the rationale, principles, and positioning of VILF within global innovation practice.

---

## 01 – Introduction

### 1.1 Purpose of the Framework
VILF addresses the need for structured, scalable, evidence-driven innovation lab ecosystems. Fragmented or isolated innovation efforts produce inconsistent results; VILF provides an architecture and operating model to build coherence and maturity across institutions.

### 1.2 Why Innovation Labs Fail
Common failure patterns include:
- unclear mandates,
- insufficient governance,
- lack of documentation discipline,
- no maturity progression pathway,
- inconsistent funding,
- unstructured experimentation,
- isolated practices with no shared learning.

VILF is explicitly designed to counter these conditions.

### 1.3 Principles of VILF
- **System Thinking:** labs are nodes in a larger ecosystem.  
- **Evidence First:** decisions require evidence, not assumptions.  
- **Governed Autonomy:** labs operate freely within IGF.  
- **Maturity Orientation:** capability evolves through IMM.  
- **Scalable Architecture:** structure must support growth.  
- **Transparency:** documentation and decisions are visible.  
- **Interoperability:** labs, hubs, NCU share the same foundations.  

### 1.4 VILF in the Global Context
VILF synthesizes global references from:
- public innovation labs,
- strategic design,
- experimentation,
- foresight,
- public-sector governance,
- innovation maturity research,
- distributed innovation models.

### 1.5 Connection to Ecosystem Diagnostic
Chapter 02 provides the diagnostic model for assessing readiness before system design begins.

---

## 02 – Ecosystem Diagnostic

### 2.1 Purpose of the Diagnostic
The diagnostic evaluates whether an institution, sector, or country is ready to deploy innovation labs, hubs, and networks using VILF. It identifies maturity levels, capability gaps, governance barriers, and funding conditions.

### 2.2 Diagnostic Pillars
The diagnostic evaluates five pillars:
- **Strategic Alignment (MCF)**  
- **Governance (IGF)**  
- **Capability & Maturity (IMM)**  
- **Execution Readiness (IMM-P)**  
- **Infrastructure & Evidence Systems**  

Each pillar has a structured set of indicators.

### 2.3 Readiness Levels
Readiness is classified as:
- Initial  
- Emerging  
- Developing  
- Established  
- Strategic  

These align with IMM maturity levels.

### 2.4 Diagnostic Methods
Includes:
- document review,
- interviews,
- capability mapping,
- systems analysis,
- evidence review,
- funding landscape analysis.

### 2.5 Diagnostic Outputs
- readiness scorecard,
- capability map,
- funding readiness profile,
- governance readiness profile,
- implementation recommendations,
- risks and mitigation pathways.

### 2.6 Minimum Conditions for Launch
A system may only proceed when:
- mandates are clear,
- funding streams identified,
- governance structures defined,
- hub architecture validated,
- documentation infrastructure prepared.

### 2.7 Connection to System Architecture
Chapter 03 uses diagnostic outputs to define the architecture of the lab ecosystem.

---

## 03 – System Architecture

### 3.1 Purpose of the System Architecture
This chapter defines the full structure of a functioning innovation lab ecosystem, including labs, hubs, NCU, evidence flows, governance architecture, documentation systems, and capability spines. It serves as the blueprint on which operations, funding, governance, and scaling are built.

### 3.2 Architectural Layers
VILF architecture includes:
- **Tier-0:** Innovation Labs  
- **Tier-1:** Hubs  
- **Tier-2:** Network Coordination Unit (NCU)  
- **Cross-cutting systems:** evidence, documentation, governance, capability, MEL.

### 3.3 Tier-0: Lab Architecture
Labs are execution units responsible for:
- research and discovery,  
- experimentation and prototyping,  
- validation cycles,  
- solution development,  
- documentation and evidence.

Minimum components:
- lab manager,  
- service/design capability,  
- experimentation capability,  
- evidence officer,  
- subject matter experts (as needed).

### 3.4 Tier-1: Hub Architecture
Hubs support multiple labs through:
- capability building,  
- coaching,  
- portfolio management,  
- MEL and benchmarking,  
- documentation quality assurance,  
- domain knowledge development.

Hubs are intermediaries between labs and the NCU.

### 3.5 Tier-2: Network Coordination Unit Architecture
The NCU governs:
- accreditation,  
- standards,  
- documentation systems,  
- governance and IGF enforcement,  
- MEL synthesis,  
- system-wide reporting.

The NCU is the backbone for interoperability and long-term sustainability.

### 3.6 Cross-Cutting Systems
Five key spines operate across all tiers:
- **Execution Spine (IMM-P)**  
- **Capability Spine (IMM)**  
- **Governance Spine (IGF)**  
- **Performance Spine (MEL)**  
- **Infrastructure Spine (documentation + evidence)**  

These spines ensure coherence across the network.

### 3.7 Evidence Flow & Repository Structure (Reconstructed)
Evidence is the central currency of VILF.  
The repository structure includes:
- research logs,  
- interview notes,  
- experimentation results,  
- prototyping documentation,  
- decision records,  
- governance escalations,  
- MEL outputs.

Evidence must:
- be standardized,  
- be tagged with metadata,  
- follow versioning rules,  
- remain auditable by hubs and NCU.

### 3.8 Infrastructure Spine (Reconstructed)
The infrastructure spine consists of:
- documentation repository,  
- knowledge base,  
- metadata standards,  
- access and permission rules,  
- platform-agnostic storage architecture.

It ensures traceability, interoperability, and transparency.

### 3.9 Governance Spine (Reconstructed)
The governance spine (IGF) defines:
- decision rights across tiers,  
- escalation pathways,  
- governance review cycles,  
- ethics and risk rules.

All architecture components must apply IGF consistently.

### 3.10 Capability Spine (Reconstructed)
The capability spine uses IMM to ensure:
- structured maturity progression,  
- capability mapping,  
- training and coaching pathways,  
- role-based competency growth,  
- performance-linked capability development.

### 3.11 Interoperability Rules Across Labs, Hubs, NCU (Reconstructed)
Interoperability requires:
- shared templates,  
- unified evidence tags,  
- standardized workflows,  
- common governance patterns,  
- shared maturity and performance definitions.

This prevents fragmentation and ensures learnings circulate.

### 3.12 Connection to Operating Model
Chapter 04 operationalizes the architecture into workflows, roles, responsibilities, and execution systems.

## 04 – Operating Model

### How to Read This Chapter
The operating model translates architectural components into daily functioning. It defines roles, workflows, execution requirements, documentation standards, and collaboration patterns. It connects the system blueprint (Chapter 03) with practical behaviors and routines.

### 4.1 Purpose of the Operating Model
The operating model defines how labs, hubs, and the NCU work every day to ensure:
- predictable execution,
- capability building,
- evidence discipline,
- governance compliance,
- MEL integration,
- cross-lab collaboration,
- documentation flow.

### 4.2 Operating Model Principles
- Evidence First  
- Execution Discipline  
- Governed Autonomy  
- Transparency  
- Learning Integration  
- Scalable Processes  
- Maturity-Aligned Operations  

### 4.3 Roles and Responsibilities (Tier-0, Tier-1, Tier-2)

#### 4.3.1 Tier-0: Lab Roles
- Lab Manager  
- Service Designer  
- Experimentation Lead  
- Evidence Officer  
- Subject Matter Specialists  

#### 4.3.2 Tier-1: Hub Roles
- Hub Coordinator  
- Capability Coaches  
- Domain Specialists  
- MEL Officers  

#### 4.3.3 Tier-2: NCU Roles
- Network Director  
- Standards & Governance Unit  
- Learning & Performance Unit  
- Infrastructure Unit  

### 4.4 IMM-P® as the Execution Spine
All lab projects must execute using:
1. Pre-Discovery  
2. Discovery  
3. Validation  
4. Solution Development  
5. Implementation Pathway  

Evidence artifacts are mandatory.

### 4.5 Workflow Architecture

#### 4.5.1 Project Intake & Prioritization
Labs receive challenges from:
- sponsoring institutions,
- ecosystem actors,
- diagnostics,
- strategic priorities.

Hubs and NCU validate alignment.

#### 4.5.2 Portfolio Management
Hubs ensure:
- balanced portfolios,
- maturity consideration,
- MEL alignment,
- risk diversification.

#### 4.5.3 Collaboration Workflow
Labs collaborate through:
- shared research,
- cross-lab squads,
- domain clusters,
- hub-led capability sessions.

### 4.6 Governance Pathways
IGF decision tiers:
- Tier-0: Lab  
- Tier-1: Hub  
- Tier-2: NCU  

Escalations follow IGF rules.

### 4.7 Evidence & Documentation Standards
Labs must use:
- standardized templates,
- evidence logs,
- decision records,
- prototype logs,
- research repositories.

### 4.8 MEL Integration
MEL provides:
- quarterly review cycles,
- annual scorecards,
- performance-based funding inputs,
- learning synthesis.

### 4.9 Capability Development
The capability spine ensures:
- IMM progression,
- coaching,
- training,
- competency building.

### 4.10 Connection to Funding Model
Chapter 05 explains how operations are financed sustainably.

---

## 05 – Funding Model

### How to Read This Chapter
This chapter defines the financial structures needed to sustain labs, hubs, and the NCU using multi-stream funding models aligned with MEL, IGF, and capability growth.

### 5.1 Purpose of the Funding Model
Funding must:
- support stability,
- ensure predictability,
- enable growth,
- align with performance,
- provide diversification,
- sustain governance and evidence systems.

### 5.2 What Must Be Funded

#### 5.2.1 Tier-0: Lab Funding
Covers:
- staffing,
- IMM-P cycles,
- research,
- prototyping,
- documentation systems.

#### 5.2.2 Tier-1: Hub Funding
Covers:
- capability building,
- portfolio management,
- shared infrastructure,
- MEL review,
- domain-specific support.

#### 5.2.3 Tier-2: Network Funding
Covers:
- governance,
- MEL synthesis,
- accreditation,
- platforms,
- transparency systems.

### 5.3 Funding Principles
- Diversification  
- Stability  
- Performance Alignment  
- Transparency  
- Reinvestment of Learning  
- Equity Across Labs  
- Outcome Orientation  

### 5.4 Multi-Stream Funding Architecture

#### 5.4.1 Public Funding Streams
- national budgets,
- program budgets,
- innovation funds,
- development banks.

#### 5.4.2 Private Sector Streams
- partnerships,
- sponsorships,
- co-financed labs,
- in-kind contributions.

#### 5.4.3 Academic Streams
- research budgets,
- grants,
- student/faculty programs.

#### 5.4.4 Philanthropic Streams
- foundations,
- civil society,
- challenge grants.

#### 5.4.5 International Cooperation
- UN agencies,
- regional bodies,
- donor programs.

### 5.5 Challenge Funds
Provide competitive, rapid funding for domain-specific experiments.

### 5.6 Pooled Funds
Multi-party contributions into shared structures.

### 5.7 Performance-Based Funding
Allocation depends on:
- IMM-P execution,
- IGF compliance,
- scorecard results,
- collaboration.

### 5.8 Funding Governance
Funding flows must follow IGF decision tiers and transparency rules.

### 5.9 Long-Term Sustainability Models
Options:
- national budget integration,
- multi-year cycles,
- endowment funds,
- PPP co-financing,
- sectoral funding clusters.

### 5.10 Connection to Benchmarking
Benchmarking ensures fair and evidence-driven funding decisions (Chapter 06).

---

## 06 – Benchmarking

### How to Read This Chapter
Benchmarking enables fair comparison, capability mapping, and evidence-based resource allocation across labs, hubs, and networks.

### 6.1 Purpose of Benchmarking
Benchmarking identifies:
- performance differentials,
- capability gaps,
- high-performing and struggling units,
- system learning opportunities.

### 6.2 Benchmarking Principles
- Normalization  
- Evidence-Based  
- Transparency  
- Comparability  
- Purpose-Driven  
- Continuous Learning  

### 6.3 Benchmarking Architecture

#### 6.3.1 Tier-0
Evaluates:
- IMM-P execution,
- evidence completeness,
- rigor,
- MEL results,
- IGF compliance.

#### 6.3.2 Tier-1
Evaluates:
- capability support,
- portfolio coordination,
- MEL review quality,
- documentation hygiene.

#### 6.3.3 Tier-2
Evaluates:
- interoperability,
- governance consistency,
- documentation flow,
- system learning.

### 6.4 Benchmarking Indicator Groups
- Execution (IMM-P)  
- Capability (IMM)  
- Governance (IGF)  
- MEL indicators  

### 6.5 Normalization Rules
Account for:
- mandates,
- maturity,
- team size,
- domain complexity.

### 6.6 Benchmarking Methods
- comparative,
- longitudinal,
- cluster-based,
- network-level.

### 6.7 Benchmarking Process Flow
1. Indicator verification  
2. Data submission  
3. Normalization  
4. Consolidation  
5. Analysis  
6. MEL integration  
7. Reporting  

### 6.8 Outputs
- benchmarking reports,
- capability plans,
- funding recommendations,
- cross-lab learning priorities.

### 6.9 Connection to KPIs & Scorecard
Benchmarking provides comparative context for KPI interpretation in Chapter 07.

---

## 07 – KPIs & Scorecard (MEL Core System)

### How to Read This Chapter
This chapter operationalizes MEL through KPIs and scorecards for labs, hubs, and the NCU.

### 7.1 Purpose
KPIs:
- measure performance,
- track maturity,
- guide funding,
- enable governance,
- standardize quality.

### 7.2 KPI Architecture
5 domains:
1. Strategic Alignment (MCF)  
2. Capability & Maturity (IMM)  
3. Execution Quality (IMM-P)  
4. Governance Compliance (IGF)  
5. Performance & Learning (MEL)  

### 7.3 KPI Catalog

#### 7.3.1 Tier-0: Lab KPIs
- L1 IMM-P stage completion  
- L2 evidence quality  
- L3 experimentation diversity  
- L4 validation strength  
- L5 documentation completeness  
- L6 time-to-insight  
- L7 collaboration  
- L8 maturity progression  
- L9 governance compliance  

#### 7.3.2 Tier-1: Hub KPIs
- H1 capability delivery  
- H2 portfolio balance  
- H3 coordination effectiveness  
- H4 MEL review  
- H5 repository quality  
- H6 domain contribution  

#### 7.3.3 Tier-2: NCU KPIs
- N1 standard adoption  
- N2 interoperability  
- N3 benchmarking cycles  
- N4 MEL synthesis  
- N5 accreditation performance  
- N6 strategic alignment impact  

### 7.4 Scoring & Weighting
Weights:
- Strategic Alignment: 15%  
- Capability & Maturity: 20%  
- Execution Quality: 30%  
- Governance: 15%  
- MEL: 20%  

### 7.5 Evidence Requirements
Each KPI requires:
- logs,
- artifacts,
- decision records,
- maturity assessments,
- MEL outputs.

### 7.6 Performance Categories
- Strategic  
- Established  
- Developing  
- Emerging  
- Initial  

### 7.7 MEL Cycles
Quarterly: light reviews  
Annual: full audit + benchmarking + funding alignment  

### 7.8 Scorecard Templates
Includes:
- KPI list,
- evidence checklist,
- scoring sheet,
- classification rules.

### 7.9 Connection to Roadmap & Phasing
Scorecards shape expansion decisions in Chapter 08.

---

## 08 – Roadmap & Phasing

### How to Read This Chapter
This chapter provides the structured rollout plan for launching and scaling innovation lab networks.

### 8.1 Purpose
Ensures:
- readiness-driven deployment,
- predictable scaling,
- maturity progression,
- MEL-driven adaptation,
- governance stability.

### 8.2 Phasing Principles
- readiness first,
- evidence over ambition,
- balanced portfolios,
- capability before scale,
- iterative learning,
- Minimum Viable Network,
- risk-managed growth.

### 8.3 Implementation Phases

#### Phase 1 — Preparation (3–6 months)
Outputs:
- diagnostics,
- architecture,
- operating model,
- funding envelope,
- MEL setup.

#### Phase 2 — Wave 1: Pilot Labs (6–12 months)
Outputs:
- pilot scorecards,
- initial hub,
- governance stabilization.

#### Phase 3 — Wave 2: Expansion (12–24 months)
Outputs:
- more labs,
- domain clusters,
- national benchmarking.

#### Phase 4 — Wave 3: National Network (24–48 months)
Outputs:
- multi-hub ecosystem,
- NCU fully operational,
- MEL institutionalized.

### 8.4 Decision Gates
- Gate 1: launch readiness  
- Gate 2: expansion readiness  
- Gate 3: integration readiness  

### 8.5 Capability Roadmap
IMM guides role competency, training, progression.

### 8.6 MEL-Driven Adaptation
Quarterly and annual MEL cycles refine the roadmap.

### 8.7 Scaling Tools
Includes:
- rollout templates,
- accreditation criteria,
- benchmarking processes.

### 8.8 Connection to Governance & Legal Toolkit
Scaling requires legal stability and governance instruments (Chapter 09).

## 09 – Governance & Legal Toolkit

### How to Read This Chapter
This chapter defines the governance, legal, and accountability systems that ensure stable operation, integrity, and long-term sustainability of innovation lab networks.

### 9.1 Purpose
Governance ensures:
- system integrity,
- decision clarity,
- risk and ethics management,
- evidence discipline,
- transparency,
- sustainability across political cycles.

### 9.2 Governance Architecture (IGF-Aligned)
Three tiers:

#### Tier-0 — Lab Governance
Responsible for:
- project decisions,
- evidence compliance,
- experimentation authorization,
- early risk identification.

#### Tier-1 — Hub Governance
Responsible for:
- cross-lab coordination,
- capability support,
- portfolio governance,
- second-level escalation.

#### Tier-2 — NCU Governance
Responsible for:
- accreditation,
- standards,
- documentation,
- MEL synthesis,
- ethics and risk oversight,
- arbitration.

### 9.3 Decision Rights & Escalation
Labs escalate to hubs when:
- risks exceed thresholds,
- cross-lab coordination is needed.

Hubs escalate to NCU when:
- institutional risks appear,
- governance violations occur.

NCU is the final decision authority.

### 9.4 Accreditation System

#### 9.4.1 Lab Accreditation
Must demonstrate:
- IMM-P compliance,
- documentation completeness,
- MEL participation,
- minimum maturity (emerging+),
- alignment with MCF.

#### 9.4.2 Hub Accreditation
Must demonstrate:
- capability-building delivery,
- portfolio governance,
- MEL competence,
- documentation hygiene.

#### 9.4.3 NCU Accreditation
Must demonstrate:
- governance maturity,
- documentation stewardship,
- MEL synthesis capability.

### 9.5 Legal Instruments

#### 9.5.1 MoUs
Define collaboration across institutions.

#### 9.5.2 Terms of Reference
Define roles, mandates, obligations.

#### 9.5.3 Decrees (Optional)
Used for formalizing national networks.

#### 9.5.4 Data-Sharing Agreements
Ensure compliance with data protection and ethics.

#### 9.5.5 IP & Licensing Frameworks
Clarify:
- publication rights,
- prototype rights,
- open knowledge rules.

#### 9.5.6 PPP Agreements
Define co-financing, shared infrastructure, joint execution.

### 9.6 Risk, Ethics & Integrity Protocols
- risk management rules,
- ethics safeguards,
- integrity expectations,
- documentation discipline.

### 9.7 Documentation & Transparency
NCU maintains:
- repository standards,
- metadata rules,
- versioning,
- decision logs,
- reporting guidelines.

### 9.8 Governance Review Cycles
Quarterly:
- governance health checks.

Annual:
- governance audits,
- accreditation cycles.

### 9.9 Connection to Templates & Tools
Chapter 10 provides governance tools, checklists, matrices, and rubrics.

---

## 10 – Templates & Tools

### How to Read This Chapter
This chapter provides the practical tools needed to implement VILF across roles and tiers.

### 10.1 Purpose
Tools ensure:
- standardization,
- documentation consistency,
- governance compliance,
- MEL integration,
- capability growth.

### 10.2 Template Domains
1. Strategic Alignment Tools (MCF)  
2. Execution Tools (IMM-P)  
3. Governance Tools (IGF)  
4. Capability Tools (IMM)  
5. Funding & Performance Tools (MEL)  
6. Legal Tools  

### 10.3 Strategic Alignment Templates
- lab mandate charter  
- challenge intake form  
- stakeholder map  

### 10.4 Execution & Evidence Templates
- pre-discovery checklist  
- research pack  
- validation pack  
- solution development pack  
- implementation pathway template  

### 10.5 Governance Templates
- decision rights matrix  
- escalation blueprint  
- governance review checklist  
- accreditation rubric  

### 10.6 Capability Templates
- IMM readiness assessment  
- capability plan  
- annual maturity reassessment  

### 10.7 Funding & Performance Templates
- KPI scorecard  
- MEL review pack  
- funding allocation template  
- challenge fund application  
- pooled fund contribution form  

### 10.8 Legal Templates
- MoU  
- ToR for labs, hubs, NCU  
- decree template  
- data-sharing agreement  
- IP template  
- PPP agreement  

### 10.9 Deployment Toolkits
- lab launch toolkit  
- hub deployment toolkit  
- NCU establishment toolkit  
- network scaling toolkit  

### 10.10 Connection to References
Chapter 11 lists the global sources that inform these tools.

---

## 11 – References

### How to Read This Chapter
This chapter lists the foundational frameworks, global references, academic sources, and governance standards that inform VILF.

### 11.1 Foundational Frameworks
- MCF 2.1  
- IMM  
- IMM-P®  
- IGF  
- MEL  

### 11.2 Innovation Lab Literature
- Nesta  
- MindLab  
- Helsinki Design Lab  
- Policy Lab UK  
- UNDP Accelerator Labs  
- OECD OPSI  
- GovLab  

### 11.3 Public-Sector Innovation & Governance
- OECD  
- European Commission  
- UNDP  
- World Bank  
- IDB  

### 11.4 Design & Systems Thinking
- IDEO  
- Stanford d.school  
- Donella Meadows  
- Eric Ries  
- Cynefin Framework  

### 11.5 Foresight & Futures
- UNESCO Futures Literacy  
- OECD Strategic Foresight  
- SOIF  
- Peter Schwartz  
- Herman Kahn  
- UN Global Pulse  
- Vigía Futura  

### 11.6 Data, Ethics & Innovation Governance
- OECD AI Principles  
- EU AI Act  
- IEEE  
- WEF  
- GovAI  

### 11.7 Academic Publications
- Harvard Kennedy School  
- MIT GOV/LAB  
- London School of Economics  
- ETH Zürich  
- University of Cambridge  

### 11.8 Supplementary Sources
- national innovation strategies  
- digital government strategies  
- governance frameworks  

### 11.9 Connection to License
Chapter 12 describes how VILF may be shared under CC BY-NC-ND 4.0.

---

## 12 – License (CC BY-NC-ND 4.0)

### How to Read This Chapter
This chapter defines the licensing rules for VILF. It has been refined to strictly match the Creative Commons Attribution–NonCommercial–NoDerivatives 4.0 International license.

### 12.1 License Model
You may:
- copy,
- share,
- redistribute the unmodified document
for non-commercial use only, with attribution.

You may not:
- modify,
- translate,
- adapt,
- create derivatives,
- use commercially.

### 12.2 Required Attribution
Must include:
“Based on the Vigía Innovation Lab Framework (VILF) by Doulab, part of the MicroCanvas Framework family. Licensed under CC BY-NC-ND 4.0.”

### 12.3 Scope
Allowed:
- internal non-commercial use,
- academic reference,
- redistribution unmodified.

Requires permission:
- translation,
- adaptation,
- national versions,
- commercial programs.

### 12.4 Permissions & Restrictions Table
Allowed?
- Copying: yes  
- Redistribution (non-commercial): yes  
- Internal government use: yes  
- Translation: no  
- Modification: no  
- National versions: no  
- Commercial use: no  
- Removing attribution: no  

### 12.5 IP Clarifications
MCF 2.1, IMM, IMM-P®, IGF, and MEL retain their protections.

### 12.6 Templates & Tools Licensing
Templates follow the same license unless third-party rules apply.

### 12.7 Derivative Works & Versioning
No derivatives allowed without permission.

### 12.8 Repository & Transparency
If published:
- must retain CC BY-NC-ND 4.0,
- no derivative PRs,
- version tagging required.

### 12.9 Cross-Framework Licensing
All Vigía frameworks share the same license model.

### 12.10 Connection to Release Notes
Next chapter documents the version history.

---

## 13 – Release Notes

### How to Read This Chapter
Release notes track all changes, versions, and compatibility notes.

### 13.1 VILF 1.0 — Initial Release
Includes:
- full architecture,
- diagnostics,
- operating model,
- governance,
- MEL,
- templates,
- annex structure.

### 13.2 Planned for VILF 1.1
- expanded MEL examples,
- additional archetypes,
- digital repository guidelines,
- domain playbooks,
- capability examples.

### 13.3 Versioning Rules
- Major: structural redesign  
- Minor: improvements  
- Patch: corrections  

### 13.4 Contributions
External suggestions allowed; modifications require permission.

---

## 14 – Roadmap (Future Versions)

### How to Read This Chapter
Outlines planned improvements across short-, medium-, and long-term horizons.

### 14.1 Purpose
Provides visibility into future enhancements.

### 14.2 Horizons
- Horizon 1 (0–12 months): refinements  
- Horizon 2 (12–24 months): archetypes, repositories  
- Horizon 3 (24+ months): cross-country models, advanced MEL  

### 14.3 Features for VILF 1.1
- expanded cases,
- role modules,
- governance audit tools,
- diagrams,
- digital tooling.

### 14.4 Candidate Features for VILF 2.0
- cross-lab data schemas,
- legislation guidance,
- portfolio simulation models,
- multi-country deployment.

### 14.5 Dependencies
- field testing,
- MEL insights,
- research,
- collaboration.

### 14.6 Governance of Roadmap
Must follow VXF versioning rules.

### 14.7 Connection to Annexes
Next chapter provides optional annexes.

---

## 15 – Annexes

### How to Read This Chapter
Annexes provide optional tools for implementation.

### 15.1 Purpose
Provide legal, operational, governance, and methodological instruments.

### 15.2 Structure
Each annex includes:
- title,
- purpose,
- scope,
- instructions,
- compatibility,
- licensing.

### 15.3 Suggested Annex List
- Annex 01: ToR — Labs  
- Annex 02: ToR — Hubs  
- Annex 03: ToR — NCU  
- Annex 04: MoU  
- Annex 05: Decision Rights Matrix  
- Annex 06: MEL Checklists  
- Annex 07: Documentation Standards  
- Annex 08: Risk & Ethics Protocols  
- Annex 09: Project Intake Form  
- Annex 10: Challenge Fund Pack  
- Annex 11: Accreditation Rubric  
- Annex 12: Capability Playbook  
- Annex 13: Lab Archetypes  
- Annex 14: National Deployment Guide  
- Annex 15: Glossary Expansion  

### 15.4 Adaptation Rules
Due to CC BY-NC-ND 4.0:
- marked annexes cannot be modified without permission.

### 15.5 Version Compatibility
Compatible with VILF 1.0 and 1.1.

### 15.6 Cross-Framework Use
Annexes may be referenced by other Vigía frameworks.

### 15.7 Closing Notes
Annexes complete the VILF 1.0 documentation suite.

