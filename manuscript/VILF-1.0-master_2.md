# Vigía Innovation Lab Framework (VILF) 1.0  
### A Framework for National and Multi-Institutional Innovation Lab Networks  
### Author: Luis Alberto Santiago Arias  
### Publisher: Doulab — Zürich  
### License: CC BY-NC-ND 4.0

---

## Table of Contents

- Chapter 00 — Executive Summary & How to Use This Framework  
- Chapter 01 — Introduction  
- Chapter 02 — Ecosystem Diagnostic  
- Chapter 03 — System Architecture  
- Chapter 04 — Operating Model  
- Chapter 05 — Funding Model  
- Chapter 06 — Benchmarking  
- Chapter 07 — KPIs & Scorecard (MEL Core System)  
- Chapter 08 — Roadmap & Phasing  
- Chapter 09 — Governance & Legal Toolkit  
- Chapter 10 — Templates & Tools  
- Chapter 11 — References  
- Chapter 12 — License  
- Chapter 13 — Release Notes  
- Chapter 14 — Framework Roadmap (Future Versions)  
- Chapter 15 — Annexes (Separate File)

---

# Chapter 00 — Executive Summary & How to Use This Framework

## 00.1 Purpose of VILF

The Vigía Innovation Lab Framework (VILF) provides a complete, implementation-ready architecture for designing, deploying, governing, scaling, and sustaining national or multi-institutional networks of innovation labs. It enables governments, universities, public–private partnerships, and distributed systems to build innovation capacity that is:

- structured  
- evidence-driven  
- governance-aligned  
- maturation-oriented  
- scalable  
- resilient across political and institutional cycles

VILF is part of the broader **Vigía Framework Family (VXF)**, which includes:

- MicroCanvas Framework (MCF)  
- Innovation Maturity Model (IMM)  
- IMM-P® (Innovation Maturity Model — Program)  
- Innovation Governance Framework (IGF)  
- Vigía Incubation Framework (VIF)  

Together, these frameworks form an integrated innovation and governance ecosystem.

## 00.2 What VILF Solves

Across governments and institutions globally, common issues persist:

- Innovation labs with unclear mandates  
- Fragmented experimentation systems  
- Lack of governance and decision pathways  
- No performance measurement or benchmarking  
- Capability gaps across teams  
- Weak or absent evidence repositories  
- Unsustainable funding structures  
- No national-level coordination  

VILF solves these by delivering:

- a unified architecture,  
- standardized processes,  
- consistent governance structures (IGF),  
- maturity pathways (IMM),  
- execution spine (IMM-P®),  
- performance measurement (MEL),  
- and sustainability models.

## 00.3 How to Use This Framework

### For Government Leaders
Use Chapters 02–05 to structure national innovation strategies and deployment plans.

### For Innovation Lab Teams
Use Chapters 03–04 and Chapter 10 for day-to-day operational guidance.

### For Hubs
Use Chapters 04, 06, and 07 to coordinate capability-building and cross-lab portfolios.

### For the Network Coordination Unit (NCU)
Use Chapters 09–14 to govern, accredit, benchmark, and sustain the network.

## 00.4 Who This Framework Is For

VILF is written for:

- Ministries and central government innovation units  
- Universities and research networks  
- Municipal innovation ecosystems  
- Multi-country regional alliances  
- Public–private innovation platforms  
- International organizations supporting innovation governance  

## 00.5 How VILF Fits Into VXF

VILF is aligned with:

- MCF 2.1 → Strategic structure  
- IMM → Maturity progression  
- IMM-P® → Execution discipline  
- IGF → Governance and decision rights  
- MEL → Evaluation, learning, and performance  

These frameworks interlock and reinforce each other across the system.

---

# Chapter 01 — Introduction

## 01.1 Why Innovation Lab Networks?

Innovation labs emerged globally as mechanisms for solving complex public and institutional challenges. However, single labs operating in isolation often struggle with:

- limited scale,  
- lack of capacity,  
- fragmented governance,  
- inconsistent evidence practices,  
- unsustainable funding,  
- uneven capability across teams.

A **network model** solves these limitations by providing:

- shared capability,  
- shared infrastructure,  
- normalized governance,  
- collective learning,  
- cross-institutional collaboration,  
- consistency across experiments,  
- and scalable national impact.

## 01.2 What Makes VILF Different

VILF is:

- **agnostic** (works for any country or institution),  
- **architectural** (not a methodology, but a system design),  
- **governance-centric** (integrates IGF directly),  
- **evidence-driven** (mandatory repositories),  
- **maturity-aware** (IMM-aligned),  
- **execution-standardized** (IMM-P®),  
- **scalable** (Wave-based expansion model).

The combination of MCF + IMM + IMM-P® + IGF + MEL produces a holistic, interoperable system.

## 01.3 Problems VILF Addresses

VILF is designed to address systemic issues including:

- Structural fragmentation  
- No standard execution model  
- Unclear mandates  
- Talent and capability gaps  
- Lack of governance  
- Weak evidence integrity  
- Lack of scaling strategy  
- No performance measurement  
- Unsustainable funding models  

## 01.4 Objectives of VILF

The framework aims to:

1. Establish a coherent architecture for lab networks.  
2. Define a maturity-based capability progression pathway.  
3. Standardize execution using IMM-P®.  
4. Create governance stability using IGF.  
5. Build performance systems using MEL & benchmarking.  
6. Provide templates and tools for operational consistency.  
7. Enable sustainable funding models.  
8. Offer a scalable national deployment strategy.  

## 01.5 Structure of VILF

VILF is organized into the following chapters:

- **Chapter 02** — Ecosystem Diagnostic  
- **Chapter 03** — System Architecture  
- **Chapter 04** — Operating Model  
- **Chapter 05** — Funding Model  
- **Chapter 06** — Benchmarking  
- **Chapter 07** — KPIs & Scorecard  
- **Chapter 08** — Roadmap & Phasing  
- **Chapter 09** — Governance & Legal Toolkit  
- **Chapter 10** — Templates & Tools  
- **Chapter 11** — References  
- **Chapter 12** — License  
- **Chapter 13** — Release Notes  
- **Chapter 14** — Roadmap  
- **Annexes** — Full implementation instruments  

## 01.6 Connection to Next Chapter

The next chapter (Chapter 02) provides a diagnostic model for assessing readiness across institutions, which is essential before deploying labs or hubs.

# Chapter 02 — Ecosystem Diagnostic

## 02.1 Purpose of the Diagnostic

Before deploying any innovation labs or hubs, a structured diagnostic is required to understand:

- institutional readiness  
- capability gaps  
- governance conditions  
- strategic alignment  
- political and operational feasibility  
- potential lab archetypes  
- resourcing and capacity  
- demand signals for innovation  

The diagnostic prevents premature deployment and ensures labs begin on solid foundations.

## 02.2 Diagnostic Principles (VXF-Aligned)

- **Evidence-Driven** — built from verified institutional data.  
- **Maturity-Based** — grounded in the Innovation Maturity Model (IMM).  
- **Strategic** — aligned with institutional priorities and mandates.  
- **Governance-Aware** — examines decision rights, risks, and escalation pathways.  
- **Capability-Focused** — identifies strengths and weaknesses in teams.  
- **Portfolio-Oriented** — evaluates demand and challenge pipelines.  
- **Scalable** — identifies which institutions can become future hubs.  

## 02.3 Diagnostic Dimensions

The VILF diagnostic examines seven dimensions:

### 02.3.1 Strategic Alignment
- clarity of institutional priorities  
- mapping of challenges to national strategies  
- leadership commitment  

### 02.3.2 Governance Conditions
- decision rights and approval complexity  
- risk management culture  
- presence of IGF-compatible structures  

### 02.3.3 Capability & Talent
- research capacity  
- design and experimentation capabilities  
- maturity of project management  
- evidence literacy  

### 02.3.4 Operational Infrastructure
- documentation systems  
- repositories  
- access to users and field contexts  
- procurement flexibility  

### 02.3.5 Maturity (IMM)
- initial → emerging → developing → established → strategic  
- readiness for lab-level processes  

### 02.3.6 Demand Signals & Portfolio Potential
- existing problems and challenges  
- sectoral opportunities  
- potential for multi-lab collaboration  

### 02.3.7 Ecosystem Conditions
- cross-sector partnerships  
- universities and research networks  
- private sector and civic engagement  

## 02.4 Diagnostic Scoring Model

Each institution receives a readiness score from **0.00 to 5.00**, categorized as:

- **Strategic (4.50–5.00)**  
- **Established (3.75–4.49)**  
- **Developing (3.00–3.74)**  
- **Emerging (2.00–2.99)**  
- **Initial (0.00–1.99)**  

A minimum **2.75** is required to participate in **Wave 1** deployments.

## 02.5 Diagnostic Outputs

The diagnostic produces:

- institutional readiness report  
- capability gap map  
- recommended lab archetype  
- recommended hub participation  
- risk map (IGF alignment)  
- preliminary portfolio overview  

## 02.6 Connection to Next Chapter

The next chapter defines the architectural configuration (labs, hubs, NCU) based on diagnostic insights.

---

# Chapter 03 — System Architecture

## 03.1 Purpose of the Architecture

The System Architecture defines how the innovation ecosystem is structured at scale. It determines:

- how labs operate within institutions,  
- how hubs coordinate multiple labs,  
- how the Network Coordination Unit (NCU) governs the system,  
- how evidence flows across tiers,  
- how capabilities are developed,  
- and how interoperability is maintained.

## 03.2 Architecture Principles (VXF Standard)

### Interoperability
All units share standards, templates, governance, and evidence conventions.

### Scalability
The system grows through structured waves and diagnostics.

### Governed Autonomy
Labs have freedom within IGF-defined constraints.

### Evidence Integrity
Documentation and repositories are mandatory.

### Capability Maturation
Skills and roles evolve through IMM pathways.

### Sustainability
Architecture supports long-term institutional integration.

## 03.3 Core Architectural Units

VILF networks are constructed through three structural tiers:

- **Tier-0 — Labs**  
- **Tier-1 — Hubs**  
- **Tier-2 — Network Coordination Unit (NCU)**  

## 03.4 Tier-0: Innovation Labs

Innovation labs are institutional units responsible for:

- research and field discovery  
- experimentation and prototyping  
- validation cycles  
- evidence generation  
- solution refinement  
- strategic recommendations  

### Typical Lab Team Roles
- Lab Manager  
- Service Designer  
- Experimentation Lead  
- Evidence Officer  
- Domain Experts  

Labs operate using the **IMM-P® Program Cycle**.

## 03.5 Tier-1: Innovation Hubs

Hubs coordinate networks of labs by providing:

- capability-building (training, coaching)  
- portfolio governance  
- documentation and repository support  
- MEL analysis  
- cross-lab collaboration structures  
- domain specialization  

Hubs serve as the "distributed backbone" of the ecosystem.

## 03.6 Tier-2: Network Coordination Unit (NCU)

The NCU governs the ecosystem by:

- maintaining standards (MCF, IMM, IMM-P®, IGF, MEL)  
- ensuring accreditation  
- conducting governance audits  
- maintaining repositories and documentation infrastructure  
- managing network-wide MEL cycles  
- producing national or multi-institutional reports  

NCU holds the highest decision rights in IGF.

## 03.7 Capability Spine

The **Capability Spine** is the distributed capacity-building model that ensures all labs and hubs develop the skills necessary for:

- research  
- experimentation  
- evidence documentation  
- governance compliance  
- MEL participation  
- strategic decision-making  

Capabilities mature along IMM pathways.

## 03.8 Infrastructure Spine

The **Infrastructure Spine** includes:

- shared documentation repositories  
- evidence platforms  
- version control systems  
- dashboards and MEL systems  
- communication infrastructure  
- secure data environments  

This ensures interoperability and consistent evidence flows.

## 03.9 Governance Architecture (IGF Integration)

The governance architecture follows IGF's tiered model:

- **Tier-0 (Labs)** — operational decisions  
- **Tier-1 (Hubs)** — cross-lab portfolio decisions  
- **Tier-2 (NCU)** — systemic governance decisions  

Escalation pathways, decision rights, and governance cycles are standardized through IGF templates.

## 03.10 Evidence Flow Architecture

Evidence flows:

1. **Lab → Hub** for review  
2. **Hub → NCU** for normalization and system learning  
3. **NCU → National/Institutional Reports**  

Evidence is stored in repositories that follow standard metadata and versioning rules.

## 03.11 Documentation Architecture

Documentation systems must support:

- interoperability  
- version control  
- metadata consistency  
- multi-institutional collaboration  
- auditability  

The NCU maintains repository standards.

## 03.12 System Interaction Diagram (Text Summary)

A textual representation of VILF system interactions:

- Labs interact with hubs via capability building and evidence submission.  
- Hubs interact with the NCU via reporting, MEL, and governance.  
- The NCU interacts with leadership via strategic reports.  
- Evidence flows vertically; learning flows horizontally.  

## 03.13 Connection to Next Chapter

The next chapter (Chapter 04) translates the architecture into day-to-day operational structures.

# Chapter 04 — Operating Model

## 04.1 Purpose of the Operating Model

The Operating Model translates the architectural design (Chapter 03) into daily operational structures. It defines:

- roles and responsibilities across tiers  
- workflows and execution pathways  
- governance processes  
- collaboration structures  
- evidence and documentation requirements  
- capability development mechanisms  
- MEL integration  

It ensures that labs, hubs, and the NCU operate coherently and predictably.

## 04.2 Operating Model Principles

### Evidence First
All decisions, iterations, and prototype changes must be backed by documented evidence in the repository.

### Execution Discipline
All labs and hubs must use the **IMM-P® Program Cycle**, ensuring standardization across the network.

### Governed Autonomy
Labs operate independently within the constraints of IGF decision rights and escalation pathways.

### Transparency
Documentation, evidence, and decision records are visible to hubs and the NCU.

### Learning Integration
MEL cycles transform evidence into system-level learning.

### Scalability
All workflows and processes remain stable as the network grows.

### Maturity Alignment
Operations scale and adapt according to IMM maturity levels.

---

## 04.3 Roles and Responsibilities

VILF defines three tiers of operational roles.

### 04.3.1 Tier-0 — Lab Roles
- **Lab Manager** — coordination, planning, IGF compliance, reporting.  
- **Service Designer** — research, journey mapping, prototyping.  
- **Experimentation Lead** — experimental design, test execution, iteration.  
- **Evidence Officer** — repository management, metadata compliance.  
- **Subject Matter Specialists** — domain expertise as needed.

### 04.3.2 Tier-1 — Hub Roles
- **Hub Coordinator** — supports multiple labs, aligns portfolios.  
- **Capability Coaches** — training, mentoring, and maturation support.  
- **Domain Specialists** — subject-specific expertise.  
- **MEL Officers** — performance and learning review.

### 04.3.3 Tier-2 — NCU Roles
- **Network Director** — strategic leadership.  
- **Standards & Governance Unit** — IGF enforcement, compliance.  
- **Learning & Performance Unit** — MEL, benchmarking.  
- **Infrastructure Unit** — repository systems, documentation standards.

---

## 04.4 IMM-P® Program Cycle (Execution Spine)

All labs follow the six-stage IMM-P® Program Cycle:

1. **Pre-Discovery**  
2. **Discovery**  
3. **Validation**  
4. **Efficiency**  
5. **Scaling**  
6. **Continuous Improvement**

Each stage has mandatory evidence artifacts stored in the Evidence Repository.

---

## 04.5 Workflow Architecture

### 04.5.1 Project Intake & Prioritization

Challenges arrive from:

- sponsoring institutions  
- network-wide priorities  
- diagnostic findings  
- sectoral strategies  

Hubs and the NCU verify alignment with:

- MCF 2.1  
- IMM-P®  
- IGF decision rights  
- MEL requirements  

### 04.5.2 Portfolio Management (Hub-Level)

Hubs coordinate portfolios across multiple labs, ensuring:

- balance of domains  
- risk distribution  
- maturity-aligned capacity  
- MEL-driven performance management  
- documentation integrity  

### 04.5.3 Collaboration Workflow

Labs collaborate through:

- shared research efforts  
- domain clusters  
- cross-lab squads  
- hub-led capability sessions  
- joint validation cycles  

---

## 04.6 Governance Pathways (IGF Integration)

IGF defines decision rights:

- **Tier-0 (Labs)** — scope, iteration, prototyping.  
- **Tier-1 (Hubs)** — portfolio alignment, cross-lab coordination.  
- **Tier-2 (NCU)** — standards, accreditation, structural governance.

Escalation follows IGF pathways.

---

## 04.7 Evidence & Documentation Standards

Labs must use:

- standard IMM-P® templates  
- evidence logs  
- decision registers  
- prototyping logs  
- research repositories  

No lab may advance to the next IMM-P® stage without the required evidence.

---

## 04.8 MEL Integration

MEL supports:

- quarterly review cycles  
- annual scorecards  
- capacity metrics  
- performance-based funding  
- cross-lab learning cycles  

Data flows:

- **Labs → Hubs → NCU**, with escalating responsibility.

---

## 04.9 Capability Development & Training

The Capability Spine ensures:

- onboarding for new labs  
- IMM-P® training  
- maturity progression  
- cross-domain upskilling  
- evidence literacy  
- governance competence  

Hubs drive capability development; the NCU sets standards.

---

## 04.10 Connection to Chapter 05

The Funding Model explains how the operating system is financed, including:

- lab-level funding  
- hub-level funding  
- NCU-level funding  
- performance-based funding  
- pooled funds  
- challenge funds  

---

# Chapter 05 — Funding Model

## 05.1 Purpose of the Funding Model

VILF requires sustained, multi-stream funding to ensure:

- labs operate consistently  
- hubs provide capability support  
- NCU maintains governance and MEL systems  
- experimentation is possible without political risk  
- the network scales predictably  

The funding model focuses on system sustainability, not one-off project financing.

---

## 05.2 What Must Be Funded

Funding is required across three levels:

### 05.2.1 Tier-0 — Labs
- team allocations  
- research and design  
- experimentation materials  
- evidence systems  
- IMM-P® cycles  

### 05.2.2 Tier-1 — Hubs
- capability-building programs  
- shared infrastructure  
- portfolio governance  
- MEL analysis  
- domain projects  

### 05.2.3 Tier-2 — NCU
- governance infrastructure  
- system-level MEL  
- accreditation  
- documentation platforms  
- communication and reporting  

---

## 05.3 Funding Principles (VXF-Aligned)

### Diversification
No single source should dominate.

### Stability
Encourage multi-year commitments.

### Performance Alignment
Funding decisions reflect MEL and benchmarking performance.

### Transparency
All funding flows must follow IGF pathways.

### Reinvestment of Learning
Evidence informs future allocations.

### Equity
Normalize funding across institutions with differing maturity levels.

---

## 05.4 Multi-Stream Funding Architecture

VILF encourages combining sources from:

### 05.4.1 Public Sector
- national/municipal budgets  
- programmatic budgets  
- research funds  
- development bank programs  

### 05.4.2 Private Sector
- co-financing  
- partnerships  
- sponsorships  
- in-kind contributions  

### 05.4.3 Academia
- research budgets  
- grants  
- applied innovation programs  

### 05.4.4 Philanthropy
- foundations  
- civil society funds  
- challenge-based grants  

### 05.4.5 International Cooperation
- UN agencies  
- regional bodies  
- development cooperation  

---

## 05.5 Challenge Funds (Optional)

Provide:

- rapid funding  
- competitive selection  
- transparent scoring  
- portfolio diversity  

Challenge funds stimulate innovation participation across sectors.

---

## 05.6 Pooled Funds

Pooled funds combine contributions from:

- public  
- private  
- academic  
- civic  

Benefits:

- shared risk  
- shared ownership  
- scalability  
- predictable funding  

---

## 05.7 Performance-Based Funding

Performance funding is tied to:

- execution quality  
- governance compliance  
- MEL and benchmarking results  
- evidence integrity  
- collaboration intensity  
- maturity progression  

---

## 05.8 Funding Flows & Governance (IGF)

### Tier-0 (Labs)
Receive operational funding through hubs or NCU.

### Tier-1 (Hubs)
Manage cross-lab allocations.

### Tier-2 (NCU)
Authorizes major allocations, sets rules, monitors compliance.

---

## 05.9 Long-Term Sustainability Models

Possible sustainability approaches:

- integrating lab budgets into national frameworks  
- multi-year cycles  
- endowments  
- public–private co-financing  
- philanthropic match models  
- regional/sectoral clusters  

---

## 05.10 Connection to Next Chapter

The next chapter (06) describes how labs, hubs, and the network are benchmarked using standardized, maturity-aware comparators.

# Chapter 06 — Benchmarking

## 06.1 Purpose of Benchmarking

Benchmarking enables the ecosystem to:

- compare labs and hubs fairly  
- identify high- and low-performing units  
- expose capability gaps  
- standardize quality  
- support MEL-driven funding  
- strengthen governance  
- inform national/systemic learning  

Benchmarking is not punitive; it is a structural mechanism for learning.

---

## 06.2 Benchmarking Principles

### Normalization First
Comparisons adjust for maturity, complexity, and resource disparities.

### Evidence-Based
Indicators rely on verified evidence in the repository.

### Transparency
Methodologies and indicators are publicly documented internally.

### Comparability
KPIs are standardized across all labs and hubs.

### Purpose-Driven
Benchmarking guides capability support, governance actions, and funding.

### Continuous Learning
Benchmarking occurs annually or biannually.

---

## 06.3 Benchmarking Architecture

VILF uses a three-tier analytical model.

### 06.3.1 Tier-0 — Lab Benchmarking
Evaluates:

- IMM-P® execution  
- evidence completeness  
- design and experimentation rigor  
- MEL results  
- IGF compliance  

### 06.3.2 Tier-1 — Hub Benchmarking
Evaluates:

- capability-building  
- portfolio management  
- MEL review quality  
- documentation integrity  
- domain leadership  

### 06.3.3 Tier-2 — Network Coordination Unit (NCU)
Evaluates:

- network-wide interoperability  
- governance consistency  
- repository quality  
- performance distribution  
- system learning generation  

---

## 06.4 Benchmarking Indicators

Indicators fall into four domains:

### 1. Execution Indicators (IMM-P®)
- stage completion rate  
- evidence quality  
- prototyping depth  
- experimentation diversity  

### 2. Capability Indicators (IMM)
- maturity progression  
- training hours  
- gap closure  

### 3. Governance Indicators (IGF)
- adherence to decision rights  
- escalation correctness  
- risk/ethics compliance  

### 4. MEL Indicators
- scorecard results  
- quarterly performance  
- repository completeness  
- learning contributions  

---

## 06.5 Normalization Rules

Normalization accounts for:

- domain complexity  
- resource variations  
- institutional mandates  
- maturity levels  

This ensures comparability across diverse labs.

---

## 06.6 Benchmarking Methods

### 06.6.1 Comparative Benchmarking
Cross-sectional comparison between units.

### 06.6.2 Longitudinal Benchmarking
Performance over time.

### 06.6.3 Cluster Benchmarking
Comparisons within sectors or domains.

### 06.6.4 Network Benchmarking
Comparisons between national networks (multi-country).

---

## 06.7 Benchmarking Process Flow

1. Indicator verification (evidence check)  
2. Data submission by labs  
3. Normalization by hubs  
4. Consolidation by NCU  
5. Comparative analysis  
6. MEL integration  
7. Reporting and feedback  

---

## 06.8 Outputs of Benchmarking

- benchmarking reports (lab/hub/network)  
- capability development plans  
- funding recommendations  
- cross-lab learning priorities  
- exemplary practice highlights  

---

## 06.9 Connection to Next Chapter

The KPI and Scorecard System (Chapter 07) operationalizes the MEL measurement framework that benchmarking relies on.

---

# Chapter 07 — KPIs & Scorecard (MEL Core System)

## 07.1 Purpose of the KPI & Scorecard System

The KPI & Scorecard system ensures:

- objective performance measurement  
- alignment with strategy and governance  
- maturation tracking (IMM)  
- evidence quality enforcement  
- performance-based funding  
- sectoral benchmarking  
- system-wide learning  

Scorecards operationalize MEL.

---

## 07.2 KPI Architecture

KPIs fall into five domains aligned with VXF:

1. **Strategic Alignment (MCF)**  
2. **Capability & Maturity (IMM)**  
3. **Execution Quality (IMM-P®)**  
4. **Governance Compliance (IGF)**  
5. **Performance & Learning (MEL)**  

---

## 07.3 KPI Catalog

### 07.3.1 Lab-Level KPIs (Tier-0)

**L1** — IMM-P® stage completion rate  
**L2** — Evidence quality index  
**L3** — Experimentation diversity  
**L4** — Validation strength  
**L5** — Documentation completeness  
**L6** — Time-to-insight  
**L7** — Collaboration intensity  
**L8** — IMM maturity progression  
**L9** — Governance compliance  

---

### 07.3.2 Hub-Level KPIs (Tier-1)

**H1** — Capability-building delivery rate  
**H2** — Portfolio balance score  
**H3** — Cross-lab coordination effectiveness  
**H4** — MEL review quality  
**H5** — Repository hygiene  
**H6** — Domain leadership contribution  

---

### 07.3.3 NCU-Level KPIs (Tier-2)

**N1** — Standard adoption rate  
**N2** — Network interoperability score  
**N3** — Benchmarking completion rate  
**N4** — MEL synthesis score  
**N5** — Accreditation cycle performance  
**N6** — Strategic alignment impact  

---

## 07.4 Scoring and Weighting Logic

Scoring Range: **0–5** per KPI  
Weights:

- Strategic Alignment — 15%  
- Capability & Maturity — 20%  
- Execution Quality — 30%  
- Governance — 15%  
- Performance & Learning — 20%  

The weighted sum produces the **Final Scorecard Index (FSI)**.

---

## 07.5 Evidence Requirements

Each KPI requires verified evidence:

- research notes  
- discovery logs  
- validation experiments  
- prototyping artifacts  
- governance decision records  
- maturity assessments  
- MEL outputs  

Scorecards are invalid without evidence.

---

## 07.6 Performance Categories

Final Scorecard Index (FSI) classifications:

- **Strategic (4.50–5.00)**  
- **Established (3.75–4.49)**  
- **Developing (3.00–3.74)**  
- **Emerging (2.00–2.99)**  
- **Initial (0.00–1.99)**  

These categories align with Benchmarking clusters.

---

## 07.7 MEL Cycles

### 07.7.1 Quarterly MEL Cycle
- KPI submission  
- evidence verification  
- hub-level MEL review  
- feedback sessions  
- light benchmarking  

### 07.7.2 Annual MEL Cycle
- full KPI audit  
- scorecard consolidation  
- benchmarking analysis  
- governance and funding recommendations  
- national or institutional MEL report  

---

## 07.8 Scorecard Templates

The standardized VILF scorecard contains:

- KPI definitions  
- evidence checklist  
- scoring sheet  
- weighting model  
- performance category classification  

---

## 07.9 Connection to Next Chapter

The next chapter (08) translates MEL signals into scaling decisions, roadmap phases, and capability interventions.

# Chapter 08 — Roadmap & Phasing

## 08.1 Purpose of the Roadmap & Phasing Model

The Roadmap provides a structured, readiness-driven sequence for deploying labs, hubs, and the national (or multi-institutional) network. It ensures:

- predictable scaling  
- governance stability  
- funding alignment  
- capability maturation  
- MEL-driven iteration  
- reduced political and operational risk  

It converts diagnostics, architecture, operations, and governance into a multi-year implementation strategy.

---

## 08.2 Phasing Principles

1. **Readiness-Driven**  
   No phase begins unless minimum readiness criteria are met.

2. **Evidence Over Ambition**  
   Expansion depends on validated learning (IMM-P®, MEL).

3. **Portfolio Balance**  
   Each wave includes a mix of domains, complexities, and maturity levels.

4. **Capability Before Scale**  
   Hubs and NCU functions must mature before expanding.

5. **Iterative Scaling**  
   Each wave improves tools, governance, and capability.

6. **Minimum Viable Network (MVN)**  
   The smallest functional configuration is deployed first.

7. **Risk-Managed Growth**  
   IGF governs escalation, integrity, and sustainability.

---

## 08.3 Standard VILF Implementation Roadmap

The roadmap has **four phases**, each with explicit objectives and outputs.

---

### **Phase 1 — Preparation & Alignment (3–6 months)**

#### Objectives
- Complete Diagnostic (Chapter 02)  
- Define architecture (Chapter 03)  
- Approve Operating Model & IGF integration  
- Secure multi-stream funding (Chapter 05)  
- Set up MEL & Benchmarking systems  

#### Outputs
- National/institutional implementation plan  
- Lab/Hub/NCU Terms of Reference  
- Initial capability-building plan  
- Funding envelope approval  

---

### **Phase 2 — Wave 1: Pilot Labs + First Hub (6–12 months)**

#### Objectives
- Launch 1–3 pilot labs  
- Operationalize first hub  
- Execute initial IMM-P® cycles  
- Test repository, templates, MEL, governance  

#### Outputs
- Pilot lab scorecards  
- Updated governance pathways  
- Initial benchmarking dataset  
- Lessons Learned Log (for Wave 2)  

---

### **Phase 3 — Wave 2: Expansion & Consolidation (12–24 months)**

#### Objectives
- Expand to 3–7 labs total  
- Strengthen existing hub or establish domain hubs  
- Institutionalize portfolio management  
- Run first full Benchmarking cycle  

#### Outputs
- National/regional benchmarking report  
- Capability progression matrix  
- Cross-lab domain clusters  

---

### **Phase 4 — Wave 3: National Network Integration (24–48 months)**

#### Objectives
- Fully operational NCU  
- Multiple functional hubs  
- Labs integrated into institutional budgets  
- Annual MEL cycles institutionalized  

#### Outputs
- Accredited labs and hubs  
- Annual network report  
- Multi-year sustainability plan  
- Alignment with national sector strategies  

---

## 08.4 Decision Gates & Readiness Checks

Each roadmap phase ends with a **readiness gate**:

### **Gate 1 — Launch Decision (Post-Phase 1)**
Requires:
- Diagnostic scores  
- Governance operational  
- Hub functional  
- Funding envelope approved  

### **Gate 2 — Expansion Decision (Post-Wave 1)**
Requires:
- Pilot MEL results  
- Scorecards  
- Updated IGF mapping  
- Documented risks resolved  

### **Gate 3 — Integration Decision (Post-Wave 2)**
Requires:
- Benchmarking results  
- Maturity progression  
- Multi-hub functionality  
- Portfolio balance indicators  

---

## 08.5 Capability-Building Roadmap (IMM-Aligned)

Maturity → Capability Pathway:

- **Initial → Emerging**  
  Fundamentals of IMM-P®, research, documentation

- **Emerging → Developing**  
  Experimentation rigor, MEL literacy

- **Developing → Established**  
  Portfolio governance, cross-lab collaboration

- **Established → Strategic**  
  Systems innovation, model refinement, domain leadership  

Hubs coordinate capability-building across labs.

---

## 08.6 MEL-Driven Adaptation Cycles

Roadmap execution incorporates learning loops:

- **Quarterly MEL Cycles:** minor course corrections  
- **Annual MEL Cycles:** strategic adjustments  
- **Benchmarking Cycles:** scaling and capability decisions  
- **IGF Governance Cycles:** risk and escalation management  

The roadmap functions as a learning system, not a fixed timeline.

---

## 08.7 Scaling Templates & Tools

VILF provides:

- rollout sequencing templates  
- lab/hub accreditation criteria  
- maturity progression matrices  
- MEL review templates  
- governance escalation models  
- portfolio management tools  

All tools appear in Chapter 10 and Annexes.

---

## 08.8 Connection to Next Chapter

Chapter 09 establishes the governance, legal, and structural frameworks required to protect and sustain the roadmap.

---

# Chapter 09 — Governance & Legal Toolkit

## 09.1 Purpose of the Governance & Legal Toolkit

This chapter provides:

- IGF-based governance structures  
- escalation pathways  
- accreditation models  
- legal instruments (MoUs, ToR, decrees, agreements)  
- risk and ethics protocols  
- transparency and documentation standards  

The toolkit ensures **system integrity** and protects the network against:

- ambiguity  
- political volatility  
- institutional fragmentation  
- inconsistent decision-making  
- documentation collapse  

---

## 09.2 Governance Architecture (IGF-Aligned)

VILF uses a three-tier governance structure:

### **Tier-0 — Lab Governance**
Handles:
- project-level decisions  
- experimentation oversight  
- evidence compliance  
- initial risk identification  

### **Tier-1 — Hub Governance**
Handles:
- portfolio alignment  
- capability support  
- MEL review quality  
- documentation enforcement  
- second-level escalation  

### **Tier-2 — Network Coordination Unit (NCU)**
Handles:
- accreditation  
- standards and documentation  
- risk & ethics review  
- final escalation/arbitration  
- transparency & reporting  

---

## 09.3 Decision Rights & Escalation Pathways

### **Lab-Level (Tier-0)**  
Labs decide on:
- research methods  
- experiments  
- prototyping iterations  
- evidence quality  

Escalate when:
- risks exceed lab mandate  
- cross-lab dependencies emerge  
- governance conflicts arise  

### **Hub-Level (Tier-1)**  
Hubs decide on:
- resource allocation  
- portfolio balancing  
- capability interventions  
- documentation quality  

Escalate when:
- institutional conflicts arise  
- sectoral risks appear  
- IGF violations occur  

### **NCU-Level (Tier-2)**  
NCU decides on:
- accreditation status  
- governance changes  
- funding eligibility  
- risk/ethics rulings  

NCU is the final authority.

## 09.4 Accreditation System

Accreditation ensures that labs, hubs, and the NCU meet the minimum standards required for participation in the network. It provides:

- quality assurance  
- maturity validation  
- performance verification  
- governance integrity  
- eligibility for funding  

### **09.4.1 Lab Accreditation Criteria**

Labs must demonstrate:

- IMM-P® compliance (all stages completed with required evidence)  
- complete and organized evidence repository  
- active MEL participation (quarterly + annual cycles)  
- defined governance roles (Tier-0)  
- maturity at least IMM Emerging  
- alignment of all projects with MCF 2.1 strategic clarity  

### **09.4.2 Hub Accreditation Criteria**

Hubs must demonstrate:

- capability-building capacity (training + coaching)  
- portfolio governance discipline  
- MEL review quality and completeness  
- domain specialization where applicable  
- repository hygiene and documentation standards  
- IGF Tier-1 compliance  

### **09.4.3 NCU Accreditation Criteria**

The NCU must demonstrate:

- system-wide governance capacity  
- accreditation cycle management  
- MEL synthesis across the network  
- documentation and evidence stewardship  
- transparency and reporting systems  

Accreditation occurs annually and is tied to funding eligibility.

---

## 09.5 Legal Instruments

VILF provides a unified legal toolkit to support collaboration, governance, and network stability.

### **09.5.1 Memoranda of Understanding (MoUs)**  
Used to formalize:

- multi-institution collaboration  
- lab/hub participation  
- shared responsibilities  
- data sharing and governance principles  

### **09.5.2 Terms of Reference (ToR)**  
Define:

- mandates  
- team composition  
- roles & responsibilities  
- reporting  
- governance rules  

Standard ToRs are provided for Labs, Hubs, and the NCU.

### **09.5.3 Decrees or Resolutions (Optional for National Models)**  
Used when formal governmental backing is required to:

- establish the NCU  
- secure long-term funding  
- mandate documentation or transparency standards  

### **09.5.4 Data Sharing & Privacy Agreements**  
Must cover:

- data access  
- privacy compliance  
- security standards  
- evidence management  
- ethical safeguards  

### **09.5.5 Intellectual Property (IP) & Licensing Frameworks**

Define rights regarding:

- prototypes  
- models  
- publications  
- joint ownership  
- dissemination  

### **09.5.6 Public–Private Partnership Agreements**

Provide structure for:

- co-financing  
- shared infrastructure  
- joint experimentation  
- public accountability  

---

## 09.6 Risk, Ethics & Integrity Protocols

Innovation involves uncertainty. VILF establishes mandatory protocols to ensure responsibility and public trust.

### **09.6.1 Risk Management Protocols**

Labs and hubs must:

- identify risks early  
- document mitigation  
- follow escalation thresholds  
- maintain a Risk Register  

### **09.6.2 Ethics Protocols**

Ensure compliance with ethical norms:

- fairness  
- transparency  
- non-discrimination  
- responsible experimentation  
- human-centered design principles  

### **09.6.3 Integrity & Accountability Protocols**

Define standards for:

- documentation integrity  
- evidence honesty  
- governance compliance  
- conflict-of-interest declarations  

NCU oversees ethics across the network.

---

## 09.7 Documentation & Transparency Standards

The NCU must maintain consistent documentation rules:

- standardized repository structure  
- evidence metadata  
- versioning and traceability  
- decision records  
- public transparency guidelines  

Labs and hubs must maintain documentation that is:

- complete  
- auditable  
- accessible (according to IGF rules)  
- aligned with MEL  

---

## 09.8 Governance Review Cycles

Two types of reviews are mandatory:

### **Quarterly Governance Reviews**

Cover:

- documentation compliance  
- evidence completeness  
- governance pathway adherence  
- early risk detection  

### **Annual Governance Audits**

Cover:

- accreditation renewal  
- MEL synthesis  
- structural governance performance  
- portfolio-level risk  
- transparency reports  

Governance audits align with MEL annual cycles.

---

## 09.9 Connection to Next Chapter

With governance, legal structures, and accountability defined, the next chapter presents **all templates and tools** needed to operationalize the entire framework.

---

# Chapter 10 — Templates & Tools

## 10.1 Purpose of the Templates & Tools Suite

Chapter 10 provides **ready-to-use operational instruments**, ensuring:

- execution consistency  
- governance integrity  
- documentation quality  
- MEL and benchmarking readiness  
- capability building  
- network-wide alignment  

Templates are structured to be copied directly by institutions.

---

## 10.2 Categories of Templates & Tools

Templates fall into six domains:

1. Strategic Alignment (MCF)  
2. Execution & Evidence (IMM-P® / IMM Program)  
3. Governance (IGF)  
4. Capability & Maturity (IMM)  
5. Funding & Performance (MEL + Chapter 05)  
6. Legal & Institutional (Chapter 09)  

---

# 10.3 Strategic Alignment Templates (MCF-Aligned)

### **10.3.1 Lab Mandate Charter**
Includes:
- purpose  
- domain  
- strategic context  
- alignment evidence  
- KPIs and success indicators  

### **10.3.2 Challenge Intake Template**
Captures:
- MCF 1.1 problem analysis  
- institutional sponsor  
- expected outcomes  
- alignment with strategic priorities  

### **10.3.3 Stakeholder Map Template**
Maps:
- influence  
- interest  
- contribution  
- engagement strategy  

---

# 10.4 Execution & Evidence Templates (IMM-P® / Innovation Maturity Model Program)

### **10.4.1 Pre-Discovery Checklist**
Assesses:
- mandate clarity  
- context alignment  
- readiness  
- IGF pathway review  

### **10.4.2 Discovery Research Pack**
Includes:
- research protocol  
- interview guides  
- evidence tags  
- synthesis templates  

### **10.4.3 Validation Pack**
Includes:
- experiment design canvas  
- validation criteria  
- results logs  
- scoring matrix  

### **10.4.4 Efficiency Pack**
Includes:
- prototype refinement logs  
- resource-to-value assessment  
- optimization insights  

### **10.4.5 Scaling Pack**
Includes:
- adoption strategy  
- ecosystem readiness  
- risk matrix  
- policy or implementation implications  

### **10.4.6 Continuous Improvement Pack**
Includes:
- retrospectives  
- evidence-of-learning  
- versioning notes  
- long-term MEL indicators  

---

# 10.5 Governance Templates (IGF-Aligned)

### **10.5.1 Decision Rights Matrix**
Defines:
- Tier-0 decisions  
- Tier-1 decisions  
- Tier-2 decisions  
- escalation triggers  

### **10.5.2 Escalation Pathways Blueprint**
Clarifies:
- when to escalate  
- thresholds  
- authority boundaries  

### **10.5.3 Governance Review Checklist**
For quarterly assessments:
- mandate alignment  
- decision compliance  
- integrity checks  

### **10.5.4 Accreditation Rubric**
Used for:
- labs  
- hubs  
- NCU  

---

# 10.6 Capability & Maturity Templates (IMM)

### **10.6.1 IMM Readiness Assessment**
Evaluates:
- maturity across five IMM dimensions  
- gaps  
- recommendations  

### **10.6.2 Capability Development Plan**
Documents:
- targeted interventions  
- training roadmap  
- coaching sessions  
- progression progress  

### **10.6.3 Annual Maturity Reassessment**
Tracks:
- progression  
- regression  
- institutional and network maturity  

---

# 10.7 Funding & Performance Templates (MEL + Funding)

### **10.7.1 KPI Scorecard Template**
Includes:
- KPI fields  
- scoring rules  
- evidence links  

### **10.7.2 MEL Review Pack**
Includes:
- quarterly review guide  
- evidence checklist  
- learning integration prompts  

### **10.7.3 Funding Allocation Template**
Aligns:
- scorecards  
- benchmarking  
- governance compliance  

### **10.7.4 Challenge Fund Application Template**
Captures:
- proposals  
- scoring  
- funding levels  

### **10.7.5 Pooled Fund Contribution Form**
Records:
- co-financing  
- commitments  
- disbursement cycles  

---

# 10.8 Legal & Institutional Templates

### **10.8.1 MoU Template**
Defines:
- roles  
- data sharing  
- governance alignment  

### **10.8.2 Terms of Reference (ToR)**
Available for:
- Labs  
- Hubs  
- NCU  

### **10.8.3 Decree Template (Optional)**
For national adoption.

### **10.8.4 Data-Sharing Agreement**
Ensures:
- privacy compliance  
- data rights  
- legal clarity  

### **10.8.5 IP Template**
Covers:
- knowledge  
- prototypes  
- shared ownership  

---

# 10.9 Toolkits for Deployment & Scaling

### **10.9.1 Lab Launch Toolkit**
Includes:
- onboarding plan  
- training sequence  
- diagnostic inputs  

### **10.9.2 Hub Deployment Toolkit**
Includes:
- hub readiness checklist  
- portfolio setup guide  
- MEL integration scripts  

### **10.9.3 NCU Establishment Toolkit**
Includes:
- governance framework  
- accreditation setup  
- transparency plan  

### **10.9.4 Network Scaling Toolkit**
Includes:
- wave templates  
- resource allocation models  
- benchmarking cycles  

---

## 10.10 Connection to Next Chapter

Chapter 11 provides all references that inform VILF, ensuring academic grounding and global alignment.

# Chapter 11 — References

## 11.1 How to Read This Chapter

Chapter 11 contains the complete list of **authoritative and academically grounded references** that inform the Vigía Innovation Lab Framework (VILF). The references are organized into thematic clusters aligned with:

- innovation frameworks  
- governance and public administration  
- design and experimentation  
- foresight and strategic anticipation  
- ethics and responsible AI  
- academic research on innovation  

All references follow **Harvard style** and are suitable for academic use, policy documentation, and institutional adoption.

---

## 11.2 Foundational Frameworks (VXF Core)

Santiago Arias, L. A. (2024) *MicroCanvas Framework 2.1: Strategic Innovation Architecture*. Doulab, Zürich.

Santiago Arias, L. A. (2024) *Innovation Maturity Model (IMM)*. Doulab, Zürich.

Santiago Arias, L. A. (2024) *IMM-P®: Innovation Maturity Model Program*. Doulab, Zürich.

Santiago Arias, L. A. (2024) *Innovation Governance Framework (IGF)*. Doulab, Zürich.

Santiago Arias, L. A. (2024) *Monitoring, Evaluation & Learning (MEL) System*. Doulab, Zürich.

Santiago Arias, L. A. (2024) *Vigía Innovation Lab Framework (VILF)*. Doulab, Zürich.

Santiago Arias, L. A. (2024) *Vigía Incubation Framework (VIF)*. Doulab, Zürich.

---

## 11.3 Global Innovation Lab Literature

Nesta (2014) *Innovation Skills: Innovation Lab Practice Guide*. London: Nesta.

Nesta (2021) *Public Sector Innovation Playbook*. London: Nesta.

MindLab (2011) *MindLab Methodology Overview*. Copenhagen: Danish Government.

Helsinki Design Lab (2013) *Legible Practices: A Design Manual*. Helsinki Design Lab.

Policy Lab UK (2020) *Policy Design Field Guide*. UK Cabinet Office.

UNDP Accelerator Labs (2019–2023) *Learning Reports and Global Insights*. New York: UNDP.

OECD OPSI (2019) *Toolkit Navigator for Public Sector Innovation*. Paris: OECD.

The GovLab (2020) *Field Scan of Public Innovation Labs*. New York University.

---

## 11.4 Public-Sector Innovation & Governance

OECD (2017) *Working with Change: Public Sector Innovation Manual*. Paris: OECD.

OECD (2018) *Toolkit for Innovation Governance*. Paris: OECD.

European Commission (2020) *Public Sector Innovation Scoreboard*. Brussels: EC.

World Bank (2018) *Government Innovation Frameworks*. Washington D.C.: World Bank.

Inter-American Development Bank (2021) *Public Innovation Labs in Latin America*. Washington D.C.: IDB.

UNDP (2021) *Innovation for Development Report*. New York: UNDP.

---

## 11.5 Design, Experimentation & Systems Thinking

IDEO (2015) *Human-Centered Design Toolkit*. IDEO.org.

Stanford d.school (2021) *Design Thinking Bootleg*. Stanford University.

Meadows, D. (2008) *Thinking in Systems: A Primer*. White River Junction: Chelsea Green Publishing.

Ries, E. (2011) *The Lean Startup*. New York: Crown Business.

Snowden, D. & Boone, M. (2007) ‘A Leader’s Framework for Decision Making’, *Harvard Business Review*.

---

## 11.6 Foresight, Futures & Strategic Anticipation

UNESCO (2021) *Futures Literacy: A New Skill for the 21st Century*. Paris: UNESCO.

OECD (2019) *Strategic Foresight for Better Policies*. Paris: OECD.

SOIF — School of International Futures (2020) *Futures Practice Guide*.

Schwartz, P. (1991) *The Art of the Long View*. New York: Doubleday.

Kahn, H. (1967) *The Year 2000*. New York: Macmillan.

UN Global Pulse (2020) *Anticipatory Systems for Public Policy*. New York: UN.

Vigía Futura (2024) *Regional Futures Index*. Doulab, Zürich.

---

## 11.7 Data, Ethics & Responsible AI Governance

OECD (2019) *OECD Principles on Artificial Intelligence*. Paris: OECD.

European Union (2024) *AI Act*. Brussels: EU.

IEEE (2019) *Ethically Aligned Design*. IEEE Standards Association.

WEF (2020) *Global Technology Governance Report*. Geneva: World Economic Forum.

GovAI (2022) *AI Governance Research Compendium*. Oxford: Centre for the Governance of AI.

---

## 11.8 Academic Research & Innovation Studies

Harvard Kennedy School (2018–2023) publications on digital government & innovation.

MIT GOV/LAB (2019–2024) research on public problem solving.

London School of Economics (LSE) (2017–2023) governance & state capability research.

ETH Zürich (2020–2024) research on innovation systems & socio-technical transitions.

University of Cambridge (2020–2024) policy innovation and strategic science studies.

---

## 11.9 Supplementary National/Regional Sources (Contextual)

These vary per deployment context and are not part of the core VILF references:

- national digital government strategies  
- innovation policy frameworks  
- civil service laws  
- public governance acts  
- regional innovation council publications  

---

## 11.10 Connection to Next Chapter

The next chapter outlines the **license** under which VILF is distributed, including attribution requirements and restrictions under **CC BY-NC-ND 4.0**.

---

# Chapter 12 — License (CC BY-NC-ND 4.0)

## 12.1 How to Read This Chapter

This chapter explains the legal terms under which VILF is distributed.  
The license governs:

- how institutions may use VILF  
- what is allowed  
- what is prohibited  
- how attribution must be provided  
- what requires explicit written permission  

The license applies to:

- all chapters (00–15)  
- all diagrams  
- all tables  
- all annexes (01–15)  
- all derivative formats (PDF, print, digital)  

No parts of VILF may be modified, translated, or adapted without written authorization from Doulab.

---

## 12.2 License Model (Official)

**VILF is licensed under:**

**Creative Commons Attribution–NonCommercial–NoDerivatives 4.0 International (CC BY-NC-ND 4.0)**

This license:

- **allows** sharing the framework in unmodified form  
- **prohibits** commercial use  
- **prohibits** modifications or translations  
- **requires** attribution  

Any exceptions require **explicit written authorization from Doulab**.

---

## 12.3 Required Attribution Text

The following text must appear **verbatim** in all redistributed copies:

> “Based on the Vigía Innovation Lab Framework (VILF) by L. A. Santiago Arias, distributed by Doulab (Santiago Arias Consulting). Licensed under CC BY-NC-ND 4.0.”

It must be included in:

- cover pages (recommended)  
- acknowledgments  
- metadata for PDFs and websites  
- annexes used in institutional settings  

---

## 12.4 Scope of Permission

### **Permitted without additional permission:**

- redistribute VILF in *unmodified* form  
- internal use by governments, universities, NGOs  
- training use (non-commercial)  
- hosting unmodified copies on internal platforms  
- linking to VILF online  

### **Not permitted without authorization:**

- modifying or adapting the framework  
- creating national versions (e.g., VILF-DR)  
- translations  
- commercial use  
- incorporating VILF into proprietary tools  
- removing attribution text  

---

## 12.5 Permissions vs. Restrictions Table

| Action | Allowed? | Notes |
|--------|----------|--------|
| Copy the framework | Yes | Must be unmodified; attribution required. |
| Redistribute publicly | Yes | Non-commercial only; attribution required. |
| Use internally (public, academic, NGO) | Yes | Internal training allowed; no modifications. |
| Translate into another language | No | Requires written authorization. |
| Modify, extend, or adapt | No | Includes structural changes, renaming, reordering. |
| Create country adaptations (e.g., VILF-DR) | No | Requires written authorization. |
| Use commercially | No | Includes paid training, consulting, or certification. |
| Remove or alter attribution | No | Attribution must remain exactly as written. |
| Integrate into paid courses | No | Unless explicitly authorized. |
| Mix with other frameworks | No | Unless authorized. |

---

## 12.6 Intellectual Property Clarifications

- All Vigía Framework components (VILF, VIF, VIGF), plus MCF 2.1, IMM, IMM-P®, IGF, and MEL, are authored by **Luis Alberto Santiago Arias** and distributed by **Doulab (Santiago Arias Consulting)**.
- These works retain full intellectual property protection.
- VILF is part of the **Vigía Framework Family (VXF)**.
- No derivatives are permitted without authorization.
- All institutional adaptations must maintain attribution.

---

## 12.7 Licensing for Templates & Tools

Templates and tools included in Chapter 10 and Annexes 01–15 are licensed under the same CC BY-NC-ND 4.0 license unless stated otherwise.

Users must:

- verify third-party toolkits (Nesta, OECD, UNDP) for extra restrictions  
- maintain attribution  
- request authorization for adaptations  

---

## 12.8 Derivative Works & Versioning Rules

Because VILF uses the **NoDerivatives (ND)** clause:

- translations = derivative works → require written permission  
- adaptations (e.g., VILF-DR) → require written permission  
- all future versions must maintain structural compatibility  
- authorized derivatives must:
  - retain attribution  
  - preserve structural integrity  
  - disclose modifications  
  - include a formal authorization notice  

Version numbers must be visible on:

- cover page  
- frontmatter  
- Docusaurus metadata  
- Release Notes  

---

## 12.9 Repository & Transparency Standards

If VILF is hosted in a public repository:

- forks must preserve the license  
- unauthorized modifications cannot be merged  
- all changes must be documented in the changelog  
- semantic versioning rules apply (X.Y.Z)  

The NCU or Doulab is the steward of the official version.

---

## 12.10 VXF Compatibility Rule

All frameworks in the Vigía family share:

- the same licensing model  
- mutual compatibility  
- shared architecture principles  
- cross-framework referencing rules  

Cross-use between frameworks is allowed **only** in unmodified form.

---

## 12.11 Connection to Next Chapter

The next chapter provides version history, compatibility notes, and release transparency for VILF.

---

# Chapter 13 — Release Notes

## 13.1 How to Read This Chapter

The Release Notes provide:

- version history  
- compatibility information  
- changes between versions  
- academic or structural updates  
- governance of the framework’s evolution  

This chapter remains short and factual for institutional clarity.

---

## 13.2 VILF 1.0 — Initial Release

**Status:** Stable  
**Version:** 1.0  
**Release Date:** To be assigned by Doulab  

### **Summary**

VILF 1.0 represents the foundational release of the Vigía Innovation Lab Framework. It includes:

- complete system design for innovation lab networks  
- architectural alignment with MCF 2.1  
- governance alignment with IGF  
- maturity alignment with IMM & IMM-P®  
- performance alignment with MEL  
- global benchmarking integration  

Chapters included:

- Cover  
- Executive Summary  
- How to Use VILF  
- Architecture Diagram  
- Glossary  
- Chapters 01–15  
- Annexes 01–15  
- References  
- License  
- Roadmap  

### **Compatibility**

- Fully compatible with VIF 1.0  
- Fully compatible with IGF, IMM, IMM-P®, MEL  
- Compatible with future VXF components  

### **Licensing Reminder**

- License: CC BY-NC-ND 4.0  
- No modifications or derivative versions without explicit authorization  
- Redistribution permitted (unmodified) with attribution  

---

## 13.3 Planned for VILF 1.1

VILF 1.1 may include:

- expanded MEL examples  
- additional innovation lab archetypes  
- advanced benchmarking indicators  
- optional digital repository standards  
- domain-specific micro-guides (health, education, mobility)  
- improved diagrams for Labs, Hubs, NCU  

### **Compatibility Note**

VILF 1.1 will remain fully backward-compatible with VILF 1.0.

---

## 13.4 Versioning Rules

All VILF versions must follow the Vigía versioning protocol:

- **Major Releases (X.0)**: Structural redesigns  
- **Minor Releases (X.Y)**: Additions, clarifications, new templates  
- **Patch Releases (X.Y.Z)**: Typos, formatting, small corrections  

Version numbers must appear visibly and consistently.

---

## 13.5 Change Request Protocol

Because VILF is ND-licensed:

- external changes cannot be incorporated  
- partners may request clarifications  
- institutional suggestions may be considered for next releases  
- Doulab approves all official changes  

---

## 13.6 Connection to Next Chapter

The next chapter outlines the development roadmap for the evolution of VILF.

---

# Chapter 14 — Roadmap (Future Evolution)

## 14.1 How to Read This Chapter

The VILF Roadmap is a forward-looking guide used for:

- institutional planning  
- national partners preparing deployments  
- researchers studying public innovation  
- cross-framework evolution planning  

The roadmap reflects priorities, not commitments.

---

## 14.2 Purpose of the VILF Roadmap

The roadmap exists to:

- ensure transparency  
- guide ecosystem contributors  
- provide visibility on upcoming improvements  
- maintain alignment across VXF frameworks  
- inform long-term planning  

The roadmap supports both national deployments and cross-national learning.

---

## 14.3 Roadmap Horizon

The VILF roadmap uses three time horizons:

### **Horizon 1 — Short Term (0–12 months)**

Planned improvements:

- refined templates and tools  
- expanded MEL examples  
- deeper diagnostic guidance  
- more benchmarking indicators  
- consistency refinements to documentation rules  

### **Horizon 2 — Medium Term (12–24 months)**

Possible additions:

- digital repository schemas  
- guidance on interoperability across lab documentation  
- archetype-specific guides (AI labs, policy labs, academic labs)  
- domain-specific playbooks (health, education, justice)  

### **Horizon 3 — Long Term (24+ months)**

Long-term evolution:

- multi-country benchmarking model  
- cross-national case library  
- advanced MEL predictive indicators  
- VXF suite integration (VILF + VIF + VIGF)  
- national legislation guidance (optional for governments)  

---

## 14.4 Candidate Features for VILF 1.1

The following items were identified during the development of VILF 1.0 but postponed to ensure release stability and clarity:

- expanded case examples for MEL  
- enhanced governance pathway diagrams  
- additional lab archetype definitions  
- expanded hub capability-building modules  
- refined documentation standards for evidence repositories  
- improved visual diagrams for Labs, Hubs, and NCU  
- optional tooling guidance (cloud-neutral, platform-neutral)  

All additions must maintain backward compatibility.

---

## 14.5 Candidate Features for VILF 2.0

A major update may introduce:

- advanced portfolio simulation tools  
- harmonized cross-framework schemas across VXF  
- multi-country scaling guidelines  
- expanded socio-technical integration models  
- predictive MEL indicators  
- refined capability maturity logic across lab types  

Major releases require extensive research and cross-institutional testing.

---

## 14.6 Dependencies for Future Releases

Future versions rely on:

- insights from national deployments  
- MEL feedback from Wave 1 and Wave 2  
- academic partnerships  
- cross-country benchmarking  
- VXF architecture evolution (VILF, VIF, VIGF)  

Updates must maintain structural coherence across the Vigía ecosystem.

---

## 14.7 Governance of the Roadmap

Roadmap changes must follow:

- CC BY-NC-ND 4.0 license constraints  
- VXF versioning rules  
- approval by Doulab  
- transparent publication in Release Notes  

The NCU (or Doulab directly) may propose roadmap updates, but final approval rests entirely with Doulab.

---

## 14.8 Connection to Next Chapter

The final chapter introduces the annex suite that supports national deployment, institutional governance, and capability building.

---

# Chapter 15 — Annexes Overview

## 15.1 How to Read This Chapter

Annexes are **optional but recommended** instruments that complement the main VILF chapters.  
They include:

- legal structures  
- documentation standards  
- governance matrices  
- capability-building tools  
- operational templates  
- deployment guides  

They must remain unmodified unless explicit authorization is granted by Doulab, as required by the CC BY-NC-ND 4.0 license.

---

## 15.2 Purpose of the Annex Suite

Annexes provide operational clarity, including:

- formal structures (ToRs, MoUs, decrees)  
- governance and decision-rights tools  
- MEL checklists and scorecards  
- evidence repository standards  
- risk, ethics, and integrity protocols  
- challenge fund governance  
- accreditation rubrics  
- capability development for hubs  
- innovation lab archetype definitions  
- national deployment guidance  

Together, they enable consistent, safe, and scalable implementation of VILF within innovation lab networks.

---

## 15.3 Annex List (Complete)

The VILF 1.0 annex suite includes:

1. ToR — Innovation Labs (Tier-0)  
2. ToR — Hubs (Tier-1)  
3. ToR — Network Coordination Unit (Tier-2)  
4. Multi-Institutional MoU Template  
5. Governance Decision Rights Matrix (IGF)  
6. MEL Checklists & Scorecards  
7. Documentation & Evidence Standards  
8. Risk & Ethics Protocols (IGF-Aligned)  
9. Innovation Project Intake & Prioritization Form  
10. Challenge Fund Governance Pack  
11. Accreditation Rubric (Lab / Hub / NCU)  
12. Hub Capability Development Playbook  
13. Innovation Lab Archetype Library  
14. National Deployment Guide  
15. Glossary Expansion  

All annexes are included in the **Annexes.md** file.

---

## 15.4 Adaptation & Authorization Rules

Due to the ND clause of the license:

- annexes cannot be modified or translated  
- national adaptations (e.g., VILF-DR) require explicit permission  
- authorized partners must include attribution and the authorization note  

---

## 15.5 Version Compatibility

- Annexes 01–15 are fully compatible with VILF 1.0  
- They are expected to remain compatible with VILF 1.1  
- Major changes may occur only in a future VILF 2.0 release  

---

## 15.6 Integration with VXF

Annexes may be referenced in:

- VILF (Labs)  
- VIF (Incubation)  
- VIGF (Governance)  
- future Vigía frameworks  

Cross-framework reuse is permitted only in unmodified form.

---

## 15.7 Closing Note

This concludes the **Main Framework** section of VILF 1.0.  
The annex suite provides the operational foundation for real-world deployment.

---

# END OF FILE — *VILF-1.0-Main-Framework.md*
